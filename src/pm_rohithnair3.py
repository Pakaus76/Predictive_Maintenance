# -*- coding: utf-8 -*-
"""PM_RohithNair3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KUSpp4X_B_VstgZLK3NSyKj1LqEgdfYt
"""

###################
### LIBRARIES DECLARATION
###################

import pandas as pd
import numpy as np
import joblib  # Library to save models
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

###################
### FUNCTIONS DEFINITIONS
###################

def data_preprocessing(file_path="CIA-1.csv"):
    """
    Perform full data preprocessing including:
    - Column renaming
    - Dropping unnecessary columns
    - Categorical to numerical conversion
    - Outlier removal (IQR & LOF)
    - Standardization
    - SMOTE for balancing classes
    - Saving preprocessed data as CSV

    Returns:
    - df_resampled: Fully processed DataFrame
    - scaler: The StandardScaler fitted to the data
    """
    ###################
    ### UPLOAD DATA
    ###################

    df = pd.read_csv(file_path)

    ###################
    ### COLUMN RENAME
    ###################

    df.rename(columns={'Air temperature [K]': 'Air Temperature',
                       'Process temperature [K]': 'Process Temperature',
                       'Rotational speed [rpm]': 'Rotational Speed',
                       'Vibration Levels ': 'Vibration Levels',
                       'Torque [Nm]': 'Torque'},
              inplace=True)

    ###################
    ### TWO COLUMN ELIMINATION
    ###################

    df.drop(['Product ID', 'UDI'], axis=1, inplace=True)

    ###################
    ### CATEGORICAL TO NUMERICAL CONVERSION (Type COLUMN)
    ###################

    df['Type'].replace({'L': 0, 'M': 1, 'H': 2}, inplace=True)

    ###################
    ### ONE-HOT ENCODING TO FAILURE TYPE
    ###################

    df['Failure'] = df['Failure Type'].apply(lambda x: 0 if x == 'No Failure' else 1)
    df.drop(columns=['Failure Type'], inplace=True)
    df.rename(columns={'Failure': 'Failure Type'}, inplace=True)

    ###################
    ### ALL TO FLOAT CONVERSION
    ###################

    df = df.astype(float)

    ###################
    ### OUTLIERS ELIMINATION
    ###################

    excluded_columns = ['Type', 'Failure Type']

    for col in df.columns:
        if col not in excluded_columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]
            if not outliers.empty:
                df.drop(outliers.index, inplace=True)

    lof = LocalOutlierFactor(n_neighbors=5)
    outliers = lof.fit_predict(df)
    df = df[outliers != -1]

    ###################
    ### STANDARDIZATION
    ###################

    columns_to_normalize = [col for col in df.columns if col not in ['Type', 'Failure Type']]
    scaler = StandardScaler()
    df_standardized = df.copy()
    df_standardized[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

    # Save the preprocessed dataset
    df_standardized.to_csv("preprocessed_data.csv", index=False)

    ###################
    ### APPLY SMOTE
    ###################

    X = df_standardized.drop(columns=["Failure Type"])
    y = df_standardized["Failure Type"]

    smote = SMOTE(sampling_strategy='auto', random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)

    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
    df_resampled["Failure Type"] = y_resampled

    # Save the balanced dataset
    df_resampled.to_csv("balanced_data.csv", index=False)

    return df_resampled, scaler


def train_evaluate_random_forest(X_train, y_train, X_test, y_test, n_estimators=200, max_depth=10, random_state=42):
    """
    Train and evaluate a Random Forest classifier.

    Parameters:
    - X_train: Training features
    - y_train: Training target variable
    - X_test: Testing features
    - y_test: Testing target variable
    - n_estimators: Number of trees (default=200)
    - max_depth: Maximum depth of the trees (default=10)
    - random_state: Random seed for reproducibility (default=42)

    Returns:
    - model: Trained Random Forest model
    - metrics: Dictionary containing Accuracy, Precision, Recall, and F1-score
    """
    rf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        class_weight="balanced"
    )

    rf.fit(X_train, y_train)
    y_pred = rf.predict(X_test)

    metrics = {
        "accuracy": accuracy_score(y_test, y_pred),
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1_score": f1_score(y_test, y_pred)
    }

    return rf, metrics


###################
### DATA PREPROCESSING FUNCTION CALL
###################

df_resampled, scaler = data_preprocessing()

###################
### DATA SELECTION & PARTITION
###################

# Select the three best features
selected_features = ["Operational Hours", "Process Temperature", "Air Temperature"]

X = df_resampled[selected_features]
y = df_resampled["Failure Type"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

###################
### RANDOM FOREST CLASSIFICATION & SAVE MODEL
###################

rf_model, rf_metrics = train_evaluate_random_forest(X_train, y_train, X_test, y_test)

# Save the trained Random Forest model
joblib.dump(rf_model, "RF_predictive_maintenance.pkl")

# Save the scaler
scaler_info = {"scaler": scaler}
joblib.dump(scaler_info, "scaler.pkl")

# Load the balanced dataset generated after preprocessing
df_resampled = pd.read_csv("balanced_data.csv")

# Compute the median values for all columns
medians = df_resampled.median().to_dict()

# Save the median values as a .pkl file
joblib.dump(medians, "medians.pkl")

print("Median values saved as 'medians.pkl'.")


# Print evaluation metrics
print("**Random Forest Performance:**")
print(f"Accuracy: {rf_metrics['accuracy']:.3f}")
print(f"Precision: {rf_metrics['precision']:.3f}")
print(f"Recall: {rf_metrics['recall']:.3f}")
print(f"F1-score: {rf_metrics['f1_score']:.3f}")

print("\nModel saved as 'RF_predictive_maintenance.pkl'")
print("Scaler saved as 'scaler.pkl'")